---
title: 'Lab 8: Count Models'
author: "Hao Wang"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document: default
  keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Reading Materials:
[R for Count Models](https://www.jstatsoft.org/article/view/v027i08)

[Interaction in Logistic Regressions](http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2009.00429.x/epdf)


# 1. Load essential packages
```{r, message=FALSE}
library(effects)
library(ggplot2)
library(MASS)
library(Zelig)
library(ZeligChoice)
library(pscl)
```



# 2. Poisson
If your DV follows a poisson distribution: 

The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume. For instance, an individual keeping track of the amount of mail they receive each day may notice that they receive an average number of 4 letters per day. 

$$f(x; \lambda) = \frac{\lambda^{x}e^{-\lambda}}{x!}$$

$$x! = x(x-1)(x-2)(x-3)...2*1$$

$\lambda$ is the mean and variance of poisson distribution. We can simulate poisson distribution in R
```{r poisson}
set.seed(1)
x <- rpois(10000, 1) #1000 poisson random numbers, lambda is 1
probability <-dpois(x, 1) 
hist(x)
```



## 2.1 Code in GLM
```{r}
mydata<-read.csv("http://j.mp/PRESenergy")
energy.poisson<-glm(Energy~rmn1173+
     grf0175+grf575+jec477+jec1177+
     jec479+embargo+hostages+oilc+
     Approval+Unemploy,
     family=poisson(link=log),
     data=mydata)
summary(energy.poisson)
```


## 2.2 Zelig
```{r}
zpoisson <- zelig(Energy~rmn1173+
     grf0175+grf575+jec477+jec1177+
     jec479+embargo+hostages+oilc+
     Approval+Unemploy, model = "poisson", data = mydata)
summary(zpoisson)
```





# 3. Negative Binomial
Definition:

Assume Bernoulli trials, that is, (1) there are two possible outcomes, (2) the trials are independent, and (3) p, the probability of success, remains the same from trial to trial. Let X denote the number of trials until the rth success. Then, the probability mass function of X is:

$$f(x; r, p) = P(X =x) = {x -1 \choose r-1}(1-p)^{x -r}p^{r} $$


Remember that the Poisson distribution assumes that the mean and variance are the same. Sometimes, your data show extra variation that is greater than the mean. This situation is called over-dispersion and negative binomial regression is more flexible in that regard than Poisson regression (you could still use Poisson regression in that case but the standard errors could be biased). The negative binomial distribution has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution.

$$f(y;\mu,\theta) = \frac{\Gamma(y+\theta)}{\Gamma(\theta)*y!} \frac{\mu^y \theta^\theta}{(\mu + \theta)^{y + \theta}}$$

## 3.1 GLM.nb
```{r}
energy.nb<-glm.nb(Energy~rmn1173+
     grf0175+grf575+jec477+jec1177+
     jec479+embargo+hostages+oilc+
     Approval+Unemploy,
     data=mydata)
summary(energy.nb)
```


## 3.2 Zelig

```{r}
znb <- zelig(Energy~rmn1173+
     grf0175+grf575+jec477+jec1177+
     jec479+embargo+hostages+oilc+
     Approval+Unemploy, model = "negbin", data = mydata)
summary(znb)
```




# 4. Zero-inflated Models
[check this](http://stats.idre.ucla.edu/r/dae/zinb/)

Zero inflated models deal with excess zero counts. They are two-component mixture models combining a point mass at zero with a count distribution such as Poisson, geometric or negative binomial. Thus, there are two sources of zeros: zeros may come from both the point mass and from the count component. For modeling the unobserved state (zero vs count), a binary model is used: in the simplest case only with an intercept but potentially containing regressors.



## 4.1 Example

The state wildlife biologists want to model how many fish are being caught by fishermen at a state park. Visitors are asked how long they stayed, how many people were in the group, were there children in the group and how many fish were caught. Some visitors do not fish, but there is no data on whether a person fished or not. Some visitors who did fish did not catch any fish so there are excess zeros in the data because of the people that did not fish

```{r, warning=FALSE}
zinb <- read.csv("http://www.ats.ucla.edu/stat/data/fish.csv")
zinb <- within(zinb, {
  nofish <- factor(nofish)
  livebait <- factor(livebait)
  camper <- factor(camper)
})

summary(zinb)
```

A zero inflated model assumes that zero outcome is due to two different processes. For instance, in the example of fishing presented here, the two processes are that a subject has gone fishing vs. not gone fishing. If not gone fishing, the only outcome possible is zero. If gone fishing, it is then a count process. The two parts of the a zero inflated model are a binary model, usually a logit model to model which of the two processes the zero outcome is associated with and a count model, in this case, a negative binomial model, to model the count process. The expected count is expressed as a combination of the two processes. We are going to use the variables child and camper to model the count in the part of negative binomial model and the variable persons in the logit part of the model.

```{r}
m1 <- zeroinfl(count ~ child + camper | persons,
  data = zinb, dist = "negbin", EM = TRUE)
summary(m1)
```








# 5. Interaction in Logistic Regressions
In linear regression we have the general interaction form:

$$\hat{Y} =\beta_0  \beta_1 X_1  + \beta_2 X_2 + \beta_{12} X_1 X_2 $$

In logistic regression, we model our desired outcome as a logistic function of latent variable $Y^*$
$$L(Y^*) = \frac{1}{1+ e^{-y^*}}$$

$$Pr(Y) = L(Y^*) = L(\beta_0  \beta_1 X_1  + \beta_2 X_2 + \beta_{12} X_1 X_2)$$
$$Pr(Y) = \frac{1}{1+e^{-(\beta_0  \beta_1 X_1  + \beta_2 X_2 + \beta_{12} X_1 X_2)}}$$


Assume our product term is meanful, the magnitude of interaction term $X_1X_2$ is the second derivative 

$$\frac{\partial^2 Pr(Y)}{\partial X_1 X_2} = Pr(Y)(1 - Pr(Y)) \beta_{12} + $$
$$Pr(Y)(1 - Pr(Y))(1-2Pr(Y)) (\beta_1 + \beta_{12} X_2 ) (\beta_2 + \beta_{12} X_1)$$

The magnitude depends on both $X_1$ and $X_2$!

Several hints:
Also read [this](https://www.cambridge.org/core/journals/political-science-research-and-methods/article/div-classtitlecompression-and-conditional-effects-a-product-term-is-essential-when-using-logistic-regression-to-test-for-interactiona-hreffn1-ref-typefnadiv/98664E6FF9AA240CCDBCFFC4B29EA7F8)

 - Logistic regression assumes interactions among variables by default.
 - The significance sign of the product term does not indicate an interaction effect
 - Instead, including interaction term adds the flexibility of the model, makes it possible for NONINTERACTION models.
 - What interaction really mean: you have theoretical expectation that there is an interaction effect between $X_1$ and $X_2$ to the latent value $Y^*$ Note: this is not equivalent to the expected outcome!
 - Plot your effect on marginal probability.











